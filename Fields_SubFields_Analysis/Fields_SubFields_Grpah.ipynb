{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dea87c7a-2820-457d-8a8e-67461e07eb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 62554 articles\n",
      "Field graph created with 26 nodes and 296 edges\n",
      "Gephi files created: field_connections.gexf and CSV files in gephi_csv/\n",
      "Subfield graph created with 241 nodes and 6990 edges\n",
      "Gephi files created: subfield_connections.gexf and CSV files in gephi_csv/\n",
      "\n",
      "Gephi Import Instructions:\n",
      "1. Open Gephi\n",
      "2. Go to File -> Open and select the .gexf file\n",
      "3. In the Import Report dialog, click OK\n",
      "4. For a clear node-edge visualization:\n",
      "   - Go to the Layout panel and apply 'ForceAtlas 2'\n",
      "   - Check 'Prevent Overlap' and click 'Run'\n",
      "   - After the layout stabilizes, click 'Stop'\n",
      "5. In the Appearance panel:\n",
      "   - To color nodes by domain: Select Nodes > Partition > domain attribute\n",
      "   - To size nodes by count: Select Nodes > Ranking > count attribute\n",
      "6. To adjust edge visibility:\n",
      "   - Select Edges > Ranking > weight\n",
      "   - Adjust the min/max size to make connections visible\n",
      "7. In Preview, select a template like 'Default Straight' to see clear edges\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# Read and parse the JSON data\n",
    "def load_data(file_path):\n",
    "    articles = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            articles.append(json.loads(line.strip()))\n",
    "    return articles\n",
    "\n",
    "# Create field connections graph\n",
    "def create_field_graph(articles):\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Track field to domain mapping\n",
    "    field_to_domain = {}\n",
    "    \n",
    "    # First, collect all fields and their domain information\n",
    "    for article in articles:\n",
    "        for topic in article['topics']:\n",
    "            if 'field' in topic and topic['field'] is not None:\n",
    "                field_name = topic['field']['display_name']\n",
    "                \n",
    "                # Store the field to domain mapping\n",
    "                if 'domain' in topic and topic['domain'] is not None:\n",
    "                    field_to_domain[field_name] = topic['domain']['display_name']\n",
    "                    \n",
    "                # Initialize node if not exists\n",
    "                if not G.has_node(field_name):\n",
    "                    G.add_node(field_name, count=0)\n",
    "    \n",
    "    # Now process each article to create connections\n",
    "    for article in articles:\n",
    "        # Extract unique fields for this article\n",
    "        fields = set()\n",
    "        for topic in article['topics']:\n",
    "            if 'field' in topic and topic['field'] is not None:\n",
    "                field_name = topic['field']['display_name']\n",
    "                fields.add(field_name)\n",
    "        \n",
    "        # Increment node counts for each field in this article\n",
    "        for field in fields:\n",
    "            G.nodes[field]['count'] += 1\n",
    "        \n",
    "        # Create connections between fields from the same article\n",
    "        fields = list(fields)\n",
    "        for i in range(len(fields)):\n",
    "            for j in range(i+1, len(fields)):\n",
    "                if G.has_edge(fields[i], fields[j]):\n",
    "                    G[fields[i]][fields[j]]['weight'] += 1\n",
    "                else:\n",
    "                    G.add_edge(fields[i], fields[j], weight=1)\n",
    "    \n",
    "    # Add domain info to each node\n",
    "    for node in G.nodes():\n",
    "        if node in field_to_domain:\n",
    "            G.nodes[node]['domain'] = field_to_domain[node]\n",
    "    \n",
    "    return G\n",
    "\n",
    "# Create subfield connections graph\n",
    "def create_subfield_graph(articles):\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Track subfield to field and domain mapping for visualization\n",
    "    subfield_to_field = {}\n",
    "    subfield_to_domain = {}\n",
    "    \n",
    "    # First, collect all subfields and their field/domain information\n",
    "    for article in articles:\n",
    "        for topic in article['topics']:\n",
    "            if 'subfield' in topic and topic['subfield'] is not None:\n",
    "                subfield_name = topic['subfield']['display_name']\n",
    "                \n",
    "                # Store the subfield to field mapping\n",
    "                if 'field' in topic and topic['field'] is not None:\n",
    "                    subfield_to_field[subfield_name] = topic['field']['display_name']\n",
    "                \n",
    "                # Store the subfield to domain mapping\n",
    "                if 'domain' in topic and topic['domain'] is not None:\n",
    "                    subfield_to_domain[subfield_name] = topic['domain']['display_name']\n",
    "                    \n",
    "                # Initialize node if not exists\n",
    "                if not G.has_node(subfield_name):\n",
    "                    G.add_node(subfield_name, count=0)\n",
    "    \n",
    "    # Now process each article to create connections\n",
    "    for article in articles:\n",
    "        # Extract unique subfields for this article\n",
    "        subfields = set()\n",
    "        for topic in article['topics']:\n",
    "            if 'subfield' in topic and topic['subfield'] is not None:\n",
    "                subfield_name = topic['subfield']['display_name']\n",
    "                subfields.add(subfield_name)\n",
    "        \n",
    "        # Increment node counts for each subfield in this article\n",
    "        for subfield in subfields:\n",
    "            G.nodes[subfield]['count'] += 1\n",
    "        \n",
    "        # Create connections between subfields from the same article\n",
    "        subfields = list(subfields)\n",
    "        for i in range(len(subfields)):\n",
    "            for j in range(i+1, len(subfields)):\n",
    "                if G.has_edge(subfields[i], subfields[j]):\n",
    "                    G[subfields[i]][subfields[j]]['weight'] += 1\n",
    "                else:\n",
    "                    G.add_edge(subfields[i], subfields[j], weight=1)\n",
    "    \n",
    "    # Add field and domain info to each node\n",
    "    for node in G.nodes():\n",
    "        if node in subfield_to_field:\n",
    "            G.nodes[node]['field'] = subfield_to_field[node]\n",
    "        if node in subfield_to_domain:\n",
    "            G.nodes[node]['domain'] = subfield_to_domain[node]\n",
    "    \n",
    "    return G\n",
    "\n",
    "# Draw network graph as a traditional node-edge graph\n",
    "def draw_network(G, title, node_size_attr='count'):\n",
    "    # Reduced figure size from 14x10 to 10x7\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    \n",
    "    # Calculate node sizes based on count - EXTREMELY reduced scaling factor\n",
    "    # Further reduced base size from 3 to 2\n",
    "    base_size = 2  # Minimum node size\n",
    "    sizes = [base_size + (G.nodes[node].get(node_size_attr, 1) * 0.2) for node in G.nodes()]\n",
    "    \n",
    "    # Calculate edge widths based on weight - EXTREMELY thin lines\n",
    "    # Further reduced max width by 20%\n",
    "    min_width = 0.1\n",
    "    max_width = 0.32  # Reduced from 0.4 (20% reduction)\n",
    "    \n",
    "    # Normalize weights for edge coloring and width\n",
    "    all_weights = [G[u][v].get('weight', 1) for u, v in G.edges()]\n",
    "    min_weight = min(all_weights) if all_weights else 1\n",
    "    max_weight = max(all_weights) if all_weights else 1\n",
    "    \n",
    "    # Create edge width and color lists\n",
    "    edge_widths = []\n",
    "    edge_colors = []\n",
    "    \n",
    "    for u, v in G.edges():\n",
    "        weight = G[u][v].get('weight', 1)\n",
    "        # Normalize weight between 0 and 1\n",
    "        norm_weight = (weight - min_weight) / (max_weight - min_weight) if max_weight > min_weight else 0.5\n",
    "        \n",
    "        # Calculate width - thinner overall\n",
    "        width = min_width + norm_weight * (max_width - min_width)\n",
    "        edge_widths.append(width)\n",
    "        \n",
    "        # Calculate color - darker for stronger connections\n",
    "        # 0.8 (light gray) to 0.0 (black)\n",
    "        color_val = 0.8 - (norm_weight * 0.8)\n",
    "        edge_colors.append((color_val, color_val, color_val))\n",
    "    \n",
    "    # Get domains for coloring\n",
    "    domains = {}\n",
    "    for node, data in G.nodes(data=True):\n",
    "        domain = data.get('domain', 'Unknown')\n",
    "        if domain not in domains:\n",
    "            domains[domain] = len(domains)\n",
    "    \n",
    "    # Create color map based on domains - fixed for Matplotlib 3.7+\n",
    "    import matplotlib as mpl\n",
    "    cmap = mpl.colormaps['tab10']\n",
    "    node_colors = [cmap(domains.get(G.nodes[node].get('domain', 'Unknown'), 0)) for node in G.nodes()]\n",
    "    \n",
    "    # Use a layout that clearly shows edges and minimizes overlaps\n",
    "    # Using spring_layout with even higher repulsion to reduce overlapping\n",
    "    pos = nx.spring_layout(G, k=0.9, seed=42)  # Increased k to 0.9 for maximum spacing\n",
    "    \n",
    "    # Draw edges with extremely thin lines and variable darkness\n",
    "    nx.draw_networkx_edges(G, pos, width=edge_widths, edge_color=edge_colors, alpha=0.6)\n",
    "    \n",
    "    # Draw nodes with clear outlines but TINY size\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=sizes, node_color=node_colors, \n",
    "                         alpha=0.7, linewidths=0.1, edgecolors='black')\n",
    "    \n",
    "    # For subfield graph, determine importance threshold for displaying text\n",
    "    if title == \"Subfield Connections\":\n",
    "        # Calculate a threshold based on node counts or degree\n",
    "        node_counts = [G.nodes[node].get(node_size_attr, 1) for node in G.nodes()]\n",
    "        node_degrees = [G.degree(node) for node in G.nodes()]\n",
    "        \n",
    "        # Choose the threshold - nodes with higher count or more connections\n",
    "        count_threshold = sorted(node_counts, reverse=True)[min(len(node_counts)//4, len(node_counts)-1)]\n",
    "        degree_threshold = sorted(node_degrees, reverse=True)[min(len(node_degrees)//4, len(node_degrees)-1)]\n",
    "        \n",
    "        # Draw labels with background only for important subfields\n",
    "        for node, (x, y) in pos.items():\n",
    "            node_count = G.nodes[node].get(node_size_attr, 1)\n",
    "            node_degree = G.degree(node)\n",
    "            \n",
    "            # Show label only if count or degree exceeds threshold\n",
    "            if node_count >= count_threshold or node_degree >= degree_threshold:\n",
    "                # Even smaller font size\n",
    "                plt.text(x + 0.01, y + 0.01, node, fontsize=2.5, fontweight=\"normal\", \n",
    "                        ha='left', va='bottom',\n",
    "                        bbox=dict(facecolor='white', alpha=0.4, boxstyle='round', pad=0.02, linewidth=0))\n",
    "    else:\n",
    "        # For field graph, draw all labels with further reduced font size\n",
    "        for node, (x, y) in pos.items():\n",
    "            # Small offset to avoid overlapping with nodes\n",
    "            offset_x = x + 0.01\n",
    "            offset_y = y + 0.01\n",
    "            plt.text(offset_x, offset_y, node, fontsize=2.5, fontweight=\"normal\", \n",
    "                    ha='left', va='bottom',  # Left alignment helps avoid node overlap\n",
    "                    bbox=dict(facecolor='white', alpha=0.4, boxstyle='round', pad=0.02, linewidth=0))\n",
    "    \n",
    "    # Add a legend for domains\n",
    "    legend_elements = []\n",
    "    for domain, idx in domains.items():\n",
    "        legend_elements.append(plt.Line2D([0], [0], marker='o', color='w', \n",
    "                              markerfacecolor=cmap(idx), markersize=4, label=domain))\n",
    "    \n",
    "    # Add a legend for edge weights\n",
    "    if max_weight > min_weight:\n",
    "        weight_legend = []\n",
    "        for i, w in enumerate([(max_weight - min_weight) * p + min_weight for p in [0.2, 0.6, 1.0]]):\n",
    "            gray_val = 0.8 - ((i * 0.4) * 0.8)\n",
    "            weight_legend.append(plt.Line2D([0], [0], color=(gray_val, gray_val, gray_val),\n",
    "                                 linewidth=min_width + (i * 0.4) * (max_width - min_width), \n",
    "                                 label=f'≈ {w:.1f}'))\n",
    "        \n",
    "        plt.legend(handles=legend_elements + weight_legend, \n",
    "                  title=\"Domains & Connection Strength\",\n",
    "                  loc=\"upper right\", fontsize=5, title_fontsize=6,\n",
    "                  framealpha=0.7)\n",
    "    else:\n",
    "        plt.legend(handles=legend_elements, title=\"Domains\", \n",
    "                  loc=\"upper right\", fontsize=5, title_fontsize=6)\n",
    "    \n",
    "    plt.title(title, fontsize=11, fontweight='bold')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save with higher DPI for better detail\n",
    "    plt.savefig(f\"{title.replace(' ', '_')}.png\", dpi=400, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    return pos\n",
    "\n",
    "# Export to Gephi format with clear node-edge structure\n",
    "def export_to_gephi(G, filename):\n",
    "    # Get min and max weights for normalization\n",
    "    all_weights = [G[u][v].get('weight', 1) for u, v in G.edges()]\n",
    "    min_weight = min(all_weights) if all_weights else 1\n",
    "    max_weight = max(all_weights) if all_weights else 1\n",
    "    \n",
    "    # Add display information to the graph for Gephi\n",
    "    for node, data in G.nodes(data=True):\n",
    "        # Store size attribute explicitly for Gephi - with extremely minimal default size\n",
    "        # Further reduced from 0.5 to 0.3\n",
    "        G.nodes[node]['viz'] = {'size': 0.3 + (data.get('count', 1) * 0.1)}  # Extremely small size\n",
    "        \n",
    "        # Add position for initial layout\n",
    "        pos = nx.spring_layout(G, k=0.9, seed=42)  # Use same layout as visualization\n",
    "        G.nodes[node]['viz']['position'] = {'x': float(pos[node][0] * 1000), \n",
    "                                           'y': float(pos[node][1] * 1000), \n",
    "                                           'z': 0.0}\n",
    "        \n",
    "        # Add color information based on domain\n",
    "        domain = data.get('domain', 'Unknown')\n",
    "        # Use a simple hash function to generate consistent colors for domains\n",
    "        import hashlib\n",
    "        domain_hash = int(hashlib.md5(domain.encode()).hexdigest(), 16)\n",
    "        r = (domain_hash & 0xFF) / 255.0\n",
    "        g = ((domain_hash >> 8) & 0xFF) / 255.0\n",
    "        b = ((domain_hash >> 16) & 0xFF) / 255.0\n",
    "        G.nodes[node]['viz']['color'] = {'r': r * 255, 'g': g * 255, 'b': b * 255, 'a': 0.7}\n",
    "    \n",
    "    # Add edge weight information to GEXF with darker colors for stronger edges\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        weight = data.get('weight', 1)\n",
    "        # Normalize weight between 0 and 1\n",
    "        norm_weight = (weight - min_weight) / (max_weight - min_weight) if max_weight > min_weight else 0.5\n",
    "        \n",
    "        # Set edge thickness to be extremely minimal\n",
    "        # Reduced max thickness by 20%\n",
    "        G[u][v]['viz'] = {'thickness': 0.05 + (norm_weight * 0.12)}\n",
    "        \n",
    "        # Calculate grayscale color - darker for stronger connections\n",
    "        # From 200 (light gray) to 0 (black)\n",
    "        gray_val = int(200 - (norm_weight * 200))\n",
    "        G[u][v]['viz']['color'] = {'r': gray_val, 'g': gray_val, 'b': gray_val, 'a': 0.6}\n",
    "    \n",
    "    # Export to GEXF (Graph Exchange XML Format)\n",
    "    nx.write_gexf(G, f\"{filename}.gexf\")\n",
    "    \n",
    "    # Create CSV files for nodes and edges\n",
    "    os.makedirs('gephi_csv', exist_ok=True)\n",
    "    \n",
    "    # Nodes CSV file\n",
    "    with open(f\"gephi_csv/{filename}_nodes.csv\", 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        # Write headers\n",
    "        writer.writerow(['Id', 'Label', 'Count', 'Domain', 'Field'])\n",
    "        \n",
    "        # Write node data\n",
    "        for node, data in G.nodes(data=True):\n",
    "            domain = data.get('domain', 'Unknown')\n",
    "            field = data.get('field', '')\n",
    "            count = data.get('count', 1)\n",
    "            \n",
    "            writer.writerow([node, node, count, domain, field])\n",
    "    \n",
    "    # Edges CSV file\n",
    "    with open(f\"gephi_csv/{filename}_edges.csv\", 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['Source', 'Target', 'Weight', 'Type'])\n",
    "        \n",
    "        for u, v, data in G.edges(data=True):\n",
    "            writer.writerow([u, v, data.get('weight', 1), 'Undirected'])\n",
    "    \n",
    "    print(f\"Gephi files created: {filename}.gexf and CSV files in gephi_csv/\")\n",
    "\n",
    "def main():\n",
    "    # Updated file path from \"paste.txt\" to \"articles_2024/all_articles.jsonl\"\n",
    "    articles = load_data(\"all_articles.jsonl\")\n",
    "    print(f\"Loaded {len(articles)} articles\")\n",
    "    \n",
    "    # Create field graph\n",
    "    field_graph = create_field_graph(articles)\n",
    "    print(f\"Field graph created with {len(field_graph.nodes())} nodes and {len(field_graph.edges())} edges\")\n",
    "    \n",
    "    # Draw field graph with clear node-edge visualization\n",
    "    draw_network(field_graph, \"Field Connections\")\n",
    "    \n",
    "    # Export field graph for Gephi\n",
    "    export_to_gephi(field_graph, \"field_connections\")\n",
    "    \n",
    "    # Create subfield graph\n",
    "    subfield_graph = create_subfield_graph(articles)\n",
    "    print(f\"Subfield graph created with {len(subfield_graph.nodes())} nodes and {len(subfield_graph.edges())} edges\")\n",
    "    \n",
    "    # Draw subfield graph with clear node-edge visualization\n",
    "    draw_network(subfield_graph, \"Subfield Connections\")\n",
    "    \n",
    "    # Export subfield graph for Gephi\n",
    "    export_to_gephi(subfield_graph, \"subfield_connections\")\n",
    "    \n",
    "    # Print instructions for Gephi\n",
    "    print(\"\\nGephi Import Instructions:\")\n",
    "    print(\"1. Open Gephi\")\n",
    "    print(\"2. Go to File -> Open and select the .gexf file\")\n",
    "    print(\"3. In the Import Report dialog, click OK\")\n",
    "    print(\"4. For a clear node-edge visualization:\")\n",
    "    print(\"   - Go to the Layout panel and apply 'ForceAtlas 2'\")\n",
    "    print(\"   - Check 'Prevent Overlap' and click 'Run'\")\n",
    "    print(\"   - After the layout stabilizes, click 'Stop'\")\n",
    "    print(\"5. In the Appearance panel:\")\n",
    "    print(\"   - To color nodes by domain: Select Nodes > Partition > domain attribute\")\n",
    "    print(\"   - To size nodes by count: Select Nodes > Ranking > count attribute\")\n",
    "    print(\"6. To adjust edge visibility:\")\n",
    "    print(\"   - Select Edges > Ranking > weight\")\n",
    "    print(\"   - Adjust the min/max size to make connections visible\")\n",
    "    print(\"7. In Preview, select a template like 'Default Straight' to see clear edges\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7472e4a-647a-425b-85a5-171eba8895f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 244026 articles\n",
      "Field graph created with 26 nodes and 317 edges\n",
      "\n",
      "Field Network Statistics:\n",
      "                        Metric             Value\n",
      "               Number of Nodes                26\n",
      "               Number of Edges               317\n",
      "                 Graph Density            0.9754\n",
      "               Min Edge Weight                 1\n",
      "               Max Edge Weight             23850\n",
      "              Mean Edge Weight          744.4006\n",
      "            Median Edge Weight          142.0000\n",
      "         Max Degree Centrality            1.0000\n",
      "        Mean Degree Centrality            0.9754\n",
      "    Max Betweenness Centrality            0.0012\n",
      "   Mean Betweenness Centrality            0.0010\n",
      " Global Clustering Coefficient            0.9769\n",
      "Number of Connected Components                 1\n",
      "        Largest Component Size                26\n",
      "  Largest Component Percentage          100.0000\n",
      "                    Min Degree                21\n",
      "                    Max Degree                25\n",
      "                   Mean Degree           24.3846\n",
      "                 Median Degree           25.0000\n",
      "             Number of Domains                 4\n",
      "            Most Common Domain Physical Sciences\n",
      "      Most Common Domain Count                10\n",
      "\n",
      "Top Nodes in Field Network:\n",
      "                              Top Nodes by Degree                                   Top Nodes by Count\n",
      "                                    Medicine (25)                                     Medicine (96297)\n",
      "Biochemistry, Genetics and Molecular Biology (25) Biochemistry, Genetics and Molecular Biology (50638)\n",
      "        Agricultural and Biological Sciences (25)                                  Engineering (45739)\n",
      "                Earth and Planetary Sciences (25)                        Environmental Science (28065)\n",
      "                       Environmental Science (25)                             Computer Science (23413)\n",
      "\n",
      "Top 20 Fields by Frequency:\n",
      "                                        Name  Count            Domain  Connections Normalized Centrality\n",
      "                                    Medicine  96297   Health Sciences           25                1.0000\n",
      "Biochemistry, Genetics and Molecular Biology  50638     Life Sciences           25                1.0000\n",
      "                                 Engineering  45739 Physical Sciences           25                1.0000\n",
      "                       Environmental Science  28065 Physical Sciences           25                1.0000\n",
      "                            Computer Science  23413 Physical Sciences           25                1.0000\n",
      "                             Social Sciences  21916   Social Sciences           25                1.0000\n",
      "                           Materials Science  19482 Physical Sciences           25                1.0000\n",
      "                       Physics and Astronomy  18788 Physical Sciences           24                0.9600\n",
      "                                Neuroscience  17831     Life Sciences           25                1.0000\n",
      "                                  Psychology  16216   Social Sciences           25                1.0000\n",
      "        Agricultural and Biological Sciences  15615     Life Sciences           25                1.0000\n",
      "                          Health Professions  12446   Health Sciences           25                1.0000\n",
      "                 Immunology and Microbiology  11989     Life Sciences           25                1.0000\n",
      "                Earth and Planetary Sciences  10805 Physical Sciences           25                1.0000\n",
      "                                   Chemistry   9252 Physical Sciences           25                1.0000\n",
      "         Economics, Econometrics and Finance   8640   Social Sciences           24                0.9600\n",
      "         Business, Management and Accounting   7243   Social Sciences           24                0.9600\n",
      "                                      Energy   5110 Physical Sciences           24                0.9600\n",
      "                           Decision Sciences   4376   Social Sciences           24                0.9600\n",
      "                                 Mathematics   4146 Physical Sciences           24                0.9600\n",
      "\n",
      "Subfield graph created with 244 nodes and 9962 edges\n",
      "\n",
      "Subfield Network Statistics:\n",
      "                        Metric             Value\n",
      "               Number of Nodes               244\n",
      "               Number of Edges              9962\n",
      "                 Graph Density            0.3360\n",
      "               Min Edge Weight                 1\n",
      "               Max Edge Weight              3874\n",
      "              Mean Edge Weight           47.5781\n",
      "            Median Edge Weight            6.0000\n",
      "         Max Degree Centrality            0.7984\n",
      "        Mean Degree Centrality            0.3360\n",
      "    Max Betweenness Centrality            0.0252\n",
      "   Mean Betweenness Centrality            0.0028\n",
      " Global Clustering Coefficient            0.6607\n",
      "Number of Connected Components                 1\n",
      "        Largest Component Size               244\n",
      "  Largest Component Percentage          100.0000\n",
      "                    Min Degree                 1\n",
      "                    Max Degree               194\n",
      "                   Mean Degree           81.6557\n",
      "                 Median Degree           81.0000\n",
      "             Number of Domains                 4\n",
      "            Most Common Domain Physical Sciences\n",
      "      Most Common Domain Count                88\n",
      "              Number of Fields                26\n",
      "             Most Common Field          Medicine\n",
      "       Most Common Field Count                38\n",
      "\n",
      "Top Nodes in Subfield Network:\n",
      "                                       Top Nodes by Degree                            Top Nodes by Count\n",
      "                                   Molecular Biology (194)                     Molecular Biology (35341)\n",
      "                              Biomedical Engineering (188)                          Epidemiology (14529)\n",
      "Public Health, Environmental and Occupational Health (184)                Biomedical Engineering (14346)\n",
      "                     Sociology and Political Science (180) Electrical and Electronic Engineering (14046)\n",
      "                             Artificial Intelligence (177)                               Surgery (13978)\n",
      "\n",
      "Top 20 Subfields by Frequency:\n",
      "                                                Name  Count            Domain  Connections Normalized Centrality                                        Field\n",
      "                                   Molecular Biology  35341     Life Sciences          194                0.7984 Biochemistry, Genetics and Molecular Biology\n",
      "                                        Epidemiology  14529   Health Sciences          159                0.6543                                     Medicine\n",
      "                              Biomedical Engineering  14346 Physical Sciences          188                0.7737                                  Engineering\n",
      "               Electrical and Electronic Engineering  14046 Physical Sciences          167                0.6872                                  Engineering\n",
      "                                             Surgery  13978   Health Sciences          155                0.6379                                     Medicine\n",
      "                                            Oncology  12934   Health Sciences          135                0.5556                                     Medicine\n",
      "                                 Materials Chemistry  12617 Physical Sciences          157                0.6461                            Materials Science\n",
      "                             Artificial Intelligence  12558 Physical Sciences          177                0.7284                             Computer Science\n",
      "                                            Genetics  12416     Life Sciences          175                0.7202 Biochemistry, Genetics and Molecular Biology\n",
      "                  Pulmonary and Respiratory Medicine  11965   Health Sciences          159                0.6543                                     Medicine\n",
      "Public Health, Environmental and Occupational Health  11556   Health Sciences          184                0.7572                                     Medicine\n",
      "                                          Physiology  10352   Health Sciences          176                0.7243                                     Medicine\n",
      "                     Sociology and Political Science  10208   Social Sciences          180                0.7407                              Social Sciences\n",
      "                                          Immunology   9032     Life Sciences          121                0.4979                  Immunology and Microbiology\n",
      "             Radiology, Nuclear Medicine and Imaging   8718   Health Sciences          166                0.6831                                     Medicine\n",
      "              Cardiology and Cardiovascular Medicine   8235   Health Sciences          135                0.5556                                     Medicine\n",
      "                                 Infectious Diseases   8219   Health Sciences          126                0.5185                                     Medicine\n",
      "                              Cognitive Neuroscience   8093     Life Sciences          160                0.6584                                 Neuroscience\n",
      "                                           Neurology   8040     Life Sciences          139                0.5720                                 Neuroscience\n",
      "                         Global and Planetary Change   8037 Physical Sciences          144                0.5926                        Environmental Science\n",
      "\n",
      "Analyzing interdisciplinary connections...\n",
      "Cross-domain connections: 237 (74.76% of all connections)\n",
      "\n",
      "Top domain connections:\n",
      "  Physical Sciences <-> Social Sciences: 58 connections\n",
      "  Life Sciences <-> Physical Sciences: 50 connections\n",
      "  Health Sciences <-> Physical Sciences: 47 connections\n",
      "  Life Sciences <-> Social Sciences: 30 connections\n",
      "  Health Sciences <-> Social Sciences: 27 connections\n",
      "\n",
      "Network Comparison Summary:\n",
      "    Network Type  Nodes  Edges Density Clustering Coefficient  Connected Components Cross-Domain Edges %\n",
      "   Field Network     26    317   0.98%                  0.98%                     1               74.76%\n",
      "Subfield Network    244   9962   0.34%                  0.66%                     1                    -\n",
      "\n",
      "All statistics have been saved to CSV and LaTeX files for use in academic publications.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Read and parse the JSON data\n",
    "def load_data(file_path):\n",
    "    articles = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            articles.append(json.loads(line.strip()))\n",
    "    return articles\n",
    "\n",
    "# Create field connections graph\n",
    "def create_field_graph(articles):\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Track field to domain mapping\n",
    "    field_to_domain = {}\n",
    "    \n",
    "    # First, collect all fields and their domain information\n",
    "    for article in articles:\n",
    "        for topic in article['topics']:\n",
    "            if 'field' in topic and topic['field'] is not None:\n",
    "                field_name = topic['field']['display_name']\n",
    "                \n",
    "                # Store the field to domain mapping\n",
    "                if 'domain' in topic and topic['domain'] is not None:\n",
    "                    field_to_domain[field_name] = topic['domain']['display_name']\n",
    "                    \n",
    "                # Initialize node if not exists\n",
    "                if not G.has_node(field_name):\n",
    "                    G.add_node(field_name, count=0)\n",
    "    \n",
    "    # Now process each article to create connections\n",
    "    for article in articles:\n",
    "        # Extract unique fields for this article\n",
    "        fields = set()\n",
    "        for topic in article['topics']:\n",
    "            if 'field' in topic and topic['field'] is not None:\n",
    "                field_name = topic['field']['display_name']\n",
    "                fields.add(field_name)\n",
    "        \n",
    "        # Increment node counts for each field in this article\n",
    "        for field in fields:\n",
    "            G.nodes[field]['count'] += 1\n",
    "        \n",
    "        # Create connections between fields from the same article\n",
    "        fields = list(fields)\n",
    "        for i in range(len(fields)):\n",
    "            for j in range(i+1, len(fields)):\n",
    "                if G.has_edge(fields[i], fields[j]):\n",
    "                    G[fields[i]][fields[j]]['weight'] += 1\n",
    "                else:\n",
    "                    G.add_edge(fields[i], fields[j], weight=1)\n",
    "    \n",
    "    # Add domain info to each node\n",
    "    for node in G.nodes():\n",
    "        if node in field_to_domain:\n",
    "            G.nodes[node]['domain'] = field_to_domain[node]\n",
    "    \n",
    "    return G\n",
    "\n",
    "# Create subfield connections graph\n",
    "def create_subfield_graph(articles):\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Track subfield to field and domain mapping for visualization\n",
    "    subfield_to_field = {}\n",
    "    subfield_to_domain = {}\n",
    "    \n",
    "    # First, collect all subfields and their field/domain information\n",
    "    for article in articles:\n",
    "        for topic in article['topics']:\n",
    "            if 'subfield' in topic and topic['subfield'] is not None:\n",
    "                subfield_name = topic['subfield']['display_name']\n",
    "                \n",
    "                # Store the subfield to field mapping\n",
    "                if 'field' in topic and topic['field'] is not None:\n",
    "                    subfield_to_field[subfield_name] = topic['field']['display_name']\n",
    "                \n",
    "                # Store the subfield to domain mapping\n",
    "                if 'domain' in topic and topic['domain'] is not None:\n",
    "                    subfield_to_domain[subfield_name] = topic['domain']['display_name']\n",
    "                    \n",
    "                # Initialize node if not exists\n",
    "                if not G.has_node(subfield_name):\n",
    "                    G.add_node(subfield_name, count=0)\n",
    "    \n",
    "    # Now process each article to create connections\n",
    "    for article in articles:\n",
    "        # Extract unique subfields for this article\n",
    "        subfields = set()\n",
    "        for topic in article['topics']:\n",
    "            if 'subfield' in topic and topic['subfield'] is not None:\n",
    "                subfield_name = topic['subfield']['display_name']\n",
    "                subfields.add(subfield_name)\n",
    "        \n",
    "        # Increment node counts for each subfield in this article\n",
    "        for subfield in subfields:\n",
    "            G.nodes[subfield]['count'] += 1\n",
    "        \n",
    "        # Create connections between subfields from the same article\n",
    "        subfields = list(subfields)\n",
    "        for i in range(len(subfields)):\n",
    "            for j in range(i+1, len(subfields)):\n",
    "                if G.has_edge(subfields[i], subfields[j]):\n",
    "                    G[subfields[i]][subfields[j]]['weight'] += 1\n",
    "                else:\n",
    "                    G.add_edge(subfields[i], subfields[j], weight=1)\n",
    "    \n",
    "    # Add field and domain info to each node\n",
    "    for node in G.nodes():\n",
    "        if node in subfield_to_field:\n",
    "            G.nodes[node]['field'] = subfield_to_field[node]\n",
    "        if node in subfield_to_domain:\n",
    "            G.nodes[node]['domain'] = subfield_to_domain[node]\n",
    "    \n",
    "    return G\n",
    "\n",
    "# Draw network graph as a traditional node-edge graph\n",
    "def draw_network(G, title, node_size_attr='count'):\n",
    "    # Differentiate figure sizes by chart type\n",
    "    if title == \"Field Connections\":\n",
    "        # Much smaller size for field charts\n",
    "        plt.figure(figsize=(1.8, 1.8))  # Extra small for fields\n",
    "    else:\n",
    "        # Slightly larger for subfields due to complexity\n",
    "        plt.figure(figsize=(2.3, 2.3))\n",
    "    \n",
    "    # Calculate node sizes based on count - extremely minimal sizing\n",
    "    base_size = 0.1  # Tiny minimum node size \n",
    "    max_size_factor = 0.005  # Drastically reduced scaling factor\n",
    "    sizes = [base_size + (G.nodes[node].get(node_size_attr, 1) * max_size_factor) for node in G.nodes()]\n",
    "    \n",
    "    # Calculate edge widths - ultra thin lines for academic print\n",
    "    min_width = 0.05\n",
    "    max_width = 0.15\n",
    "    \n",
    "    # Normalize weights for edge coloring and width\n",
    "    all_weights = [G[u][v].get('weight', 1) for u, v in G.edges()]\n",
    "    min_weight = min(all_weights) if all_weights else 1\n",
    "    max_weight = max(all_weights) if all_weights else 1\n",
    "    \n",
    "    # Create edge width and color lists\n",
    "    edge_widths = []\n",
    "    edge_colors = []\n",
    "    \n",
    "    for u, v in G.edges():\n",
    "        weight = G[u][v].get('weight', 1)\n",
    "        # Normalize weight between 0 and 1\n",
    "        norm_weight = (weight - min_weight) / (max_weight - min_weight) if max_weight > min_weight else 0.5\n",
    "        \n",
    "        # Calculate width - extra thin for print\n",
    "        width = min_width + norm_weight * (max_width - min_width)\n",
    "        edge_widths.append(width)\n",
    "        \n",
    "        # Calculate color - darker for stronger connections\n",
    "        # Use grayscale for better print reproduction\n",
    "        color_val = 0.7 - (norm_weight * 0.7)\n",
    "        edge_colors.append((color_val, color_val, color_val))\n",
    "    \n",
    "    # Get domains for coloring\n",
    "    domains = {}\n",
    "    for node, data in G.nodes(data=True):\n",
    "        domain = data.get('domain', 'Unknown')\n",
    "        if domain not in domains:\n",
    "            domains[domain] = len(domains)\n",
    "    \n",
    "    # Create color map based on domains - using colorblind-friendly palette\n",
    "    import matplotlib as mpl\n",
    "    cmap = mpl.colormaps['tab10']\n",
    "    node_colors = [cmap(domains.get(G.nodes[node].get('domain', 'Unknown'), 0)) for node in G.nodes()]\n",
    "    \n",
    "    # Use a layout with much higher k value for better node separation\n",
    "    pos = nx.spring_layout(G, k=2.5, iterations=200, seed=42)\n",
    "    \n",
    "    # Draw edges first with thin lines\n",
    "    nx.draw_networkx_edges(G, pos, width=edge_widths, edge_color=edge_colors, alpha=0.5)\n",
    "    \n",
    "    # Draw nodes with minimal size and very low alpha for less visual dominance\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=sizes, node_color=node_colors, \n",
    "                         alpha=0.5, linewidths=0.03, edgecolors='black')\n",
    "    \n",
    "    # More strategic label display with priority for distant nodes\n",
    "    # First identify center of graph\n",
    "    center_x = sum(x for x, y in pos.values()) / len(pos)\n",
    "    center_y = sum(y for x, y in pos.values()) / len(pos)\n",
    "    \n",
    "    # Calculate distances from center for each node\n",
    "    distances = {node: np.sqrt((x - center_x)**2 + (y - center_y)**2) for node, (x, y) in pos.items()}\n",
    "    \n",
    "    # For subfield graph, be much more selective with labels\n",
    "    if title == \"Subfield Connections\":\n",
    "        # Calculate importance factors\n",
    "        node_counts = [G.nodes[node].get(node_size_attr, 1) for node in G.nodes()]\n",
    "        node_degrees = [G.degree(node) for node in G.nodes()]\n",
    "        \n",
    "        # Set very restrictive thresholds - only top ~5% by count/degree, top ~15% by distance\n",
    "        count_threshold = sorted(node_counts, reverse=True)[min(len(node_counts)//20, len(node_counts)-1)]\n",
    "        degree_threshold = sorted(node_degrees, reverse=True)[min(len(node_degrees)//20, len(node_degrees)-1)]\n",
    "        distance_threshold = sorted(distances.values(), reverse=True)[min(len(distances)//7, len(distances)-1)]\n",
    "        \n",
    "        # Draw labels for only the most important or very distant nodes\n",
    "        for node, (x, y) in pos.items():\n",
    "            node_count = G.nodes[node].get(node_size_attr, 1)\n",
    "            node_degree = G.degree(node)\n",
    "            node_distance = distances[node]\n",
    "            \n",
    "            # Label if it's very important OR if it's very distant\n",
    "            if (node_count >= count_threshold or node_degree >= degree_threshold or \n",
    "                node_distance >= distance_threshold):\n",
    "                # Smaller font for subfields\n",
    "                plt.text(x, y, node, fontsize=1.8, ha='center', va='center',\n",
    "                        bbox=dict(facecolor='white', alpha=0.9, boxstyle='round', \n",
    "                              pad=0.3, linewidth=0))\n",
    "    else:\n",
    "        # For field graph, show more labels with priority for distant nodes\n",
    "        node_counts = [G.nodes[node].get(node_size_attr, 1) for node in G.nodes()]\n",
    "        count_threshold = sorted(node_counts, reverse=True)[min(len(node_counts)//6, len(node_counts)-1)]\n",
    "        distance_threshold = sorted(distances.values(), reverse=True)[min(len(distances)//3, len(distances)-1)]\n",
    "        \n",
    "        for node, (x, y) in pos.items():\n",
    "            node_count = G.nodes[node].get(node_size_attr, 1)\n",
    "            node_distance = distances[node]\n",
    "            \n",
    "            # Label if it's important OR if it's distant\n",
    "            if node_count >= count_threshold or node_distance >= distance_threshold:\n",
    "                # Larger font for fields\n",
    "                plt.text(x, y, node, fontsize=2.5, ha='center', va='center',\n",
    "                        bbox=dict(facecolor='white', alpha=0.9, boxstyle='round', \n",
    "                              pad=0.3, linewidth=0))\n",
    "    \n",
    "    # Create more comprehensive legend showing more domains\n",
    "    # Show more of the top domains to ensure coverage\n",
    "    top_domains = sorted([(domain, sum(1 for _, data in G.nodes(data=True) \n",
    "                        if data.get('domain', 'Unknown') == domain)) \n",
    "                       for domain in domains], key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Include at least 5 domains, more if there are ties at the cutoff\n",
    "    cutoff_count = top_domains[min(4, len(top_domains)-1)][1]\n",
    "    top_domains = [d for d in top_domains if d[1] >= cutoff_count][:8]  # Cap at 8 domains max\n",
    "    \n",
    "    legend_elements = []\n",
    "    for domain, _ in top_domains:\n",
    "        idx = domains[domain]\n",
    "        # Full domain names for better readability\n",
    "        legend_elements.append(plt.Line2D([0], [0], marker='o', color='w', \n",
    "                             markerfacecolor=cmap(idx), markersize=3, label=domain))\n",
    "    \n",
    "    # Two weight elements for better clarity\n",
    "    weight_legend = [\n",
    "        plt.Line2D([0], [0], color='lightgray', linewidth=min_width, label='Weak Link'),\n",
    "        plt.Line2D([0], [0], color='darkgray', linewidth=max_width, label='Strong Link')\n",
    "    ]\n",
    "    \n",
    "    # Always position legend at the bottom center\n",
    "    plt.legend(handles=legend_elements + weight_legend, \n",
    "             loc=\"lower center\", bbox_to_anchor=(0.5, -0.15),\n",
    "             ncol=2, fontsize=3.5, title_fontsize=0,\n",
    "             framealpha=0.8, borderpad=0.4, labelspacing=0.3,\n",
    "             handletextpad=0.4, handlelength=1.0)\n",
    "    \n",
    "    plt.title(title, fontsize=8, pad=5)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save as high-resolution vector graphics for publication\n",
    "    plt.savefig(f\"{title.replace(' ', '_')}.pdf\", format='pdf', bbox_inches='tight', dpi=1200)\n",
    "    # Also save PNG for quick reference with higher DPI\n",
    "    plt.savefig(f\"{title.replace(' ', '_')}.png\", dpi=1200, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    return pos\n",
    "\n",
    "# Generate top fields/subfields by frequency\n",
    "def generate_top_nodes_table(G, title, count_attr='count', top_n=20):\n",
    "    \"\"\"\n",
    "    Generate a table of the top n nodes by frequency (count attribute).\n",
    "    Returns a DataFrame with node names, counts, and additional attributes.\n",
    "    \"\"\"\n",
    "    # Get all nodes with their counts\n",
    "    node_data = []\n",
    "    for node, data in G.nodes(data=True):\n",
    "        count = data.get(count_attr, 0)\n",
    "        domain = data.get('domain', 'Unknown')\n",
    "        # Include field if it exists (for subfields)\n",
    "        field = data.get('field', '') if 'field' in data else ''\n",
    "        degree = G.degree(node)\n",
    "        \n",
    "        node_info = {\n",
    "            'Name': node,\n",
    "            'Count': count,\n",
    "            'Domain': domain,\n",
    "            'Connections': degree,\n",
    "            'Normalized Centrality': nx.degree_centrality(G)[node]\n",
    "        }\n",
    "        \n",
    "        if field:\n",
    "            node_info['Field'] = field\n",
    "            \n",
    "        node_data.append(node_info)\n",
    "    \n",
    "    # Convert to DataFrame and sort by count\n",
    "    df = pd.DataFrame(node_data)\n",
    "    df = df.sort_values('Count', ascending=False).head(top_n)\n",
    "    \n",
    "    # Format the normalized centrality column\n",
    "    df['Normalized Centrality'] = df['Normalized Centrality'].apply(lambda x: f\"{x:.4f}\")\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(f\"Top_{top_n}_{title.replace(' ', '_')}.csv\", index=False)\n",
    "    \n",
    "    # Create LaTeX table\n",
    "    latex_table = df.to_latex(index=False, \n",
    "                             caption=f\"Top {top_n} {title} by Frequency\", \n",
    "                             label=f\"tab:top_{top_n}_{title.replace(' ', '_').lower()}\",\n",
    "                             longtable=True, escape=False)\n",
    "    \n",
    "    with open(f\"Top_{top_n}_{title.replace(' ', '_')}.tex\", 'w') as f:\n",
    "        f.write(latex_table)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate network statistics for academic publication\n",
    "def generate_network_statistics(G, title):\n",
    "    \"\"\"\n",
    "    Generate comprehensive network statistics for academic publications.\n",
    "    Returns a DataFrame with key metrics.\n",
    "    \"\"\"\n",
    "    stats = {}\n",
    "    \n",
    "    # Basic graph metrics\n",
    "    stats['Number of Nodes'] = len(G.nodes())\n",
    "    stats['Number of Edges'] = len(G.edges())\n",
    "    stats['Graph Density'] = nx.density(G)\n",
    "    \n",
    "    # Get all weights\n",
    "    weights = [G[u][v].get('weight', 1) for u, v in G.edges()]\n",
    "    \n",
    "    # Edge weight statistics\n",
    "    stats['Min Edge Weight'] = min(weights) if weights else 0\n",
    "    stats['Max Edge Weight'] = max(weights) if weights else 0\n",
    "    stats['Mean Edge Weight'] = np.mean(weights) if weights else 0\n",
    "    stats['Median Edge Weight'] = np.median(weights) if weights else 0\n",
    "    \n",
    "    # Centrality measures\n",
    "    degree_centrality = nx.degree_centrality(G)\n",
    "    stats['Max Degree Centrality'] = max(degree_centrality.values()) if degree_centrality else 0\n",
    "    stats['Mean Degree Centrality'] = np.mean(list(degree_centrality.values())) if degree_centrality else 0\n",
    "    \n",
    "    betweenness_centrality = nx.betweenness_centrality(G)\n",
    "    stats['Max Betweenness Centrality'] = max(betweenness_centrality.values()) if betweenness_centrality else 0\n",
    "    stats['Mean Betweenness Centrality'] = np.mean(list(betweenness_centrality.values())) if betweenness_centrality else 0\n",
    "    \n",
    "    # Clustering\n",
    "    stats['Global Clustering Coefficient'] = nx.average_clustering(G)\n",
    "    \n",
    "    # Connected components\n",
    "    connected_components = list(nx.connected_components(G))\n",
    "    stats['Number of Connected Components'] = len(connected_components)\n",
    "    \n",
    "    largest_cc = max(connected_components, key=len)\n",
    "    stats['Largest Component Size'] = len(largest_cc)\n",
    "    stats['Largest Component Percentage'] = (len(largest_cc) / len(G.nodes())) * 100\n",
    "    \n",
    "    # Degree distribution statistics\n",
    "    degrees = [d for _, d in G.degree()]\n",
    "    stats['Min Degree'] = min(degrees) if degrees else 0\n",
    "    stats['Max Degree'] = max(degrees) if degrees else 0\n",
    "    stats['Mean Degree'] = np.mean(degrees) if degrees else 0\n",
    "    stats['Median Degree'] = np.median(degrees) if degrees else 0\n",
    "    \n",
    "    # Domain diversity\n",
    "    domains = [data.get('domain', 'Unknown') for _, data in G.nodes(data=True)]\n",
    "    domain_counts = Counter(domains)\n",
    "    stats['Number of Domains'] = len(domain_counts)\n",
    "    stats['Most Common Domain'] = domain_counts.most_common(1)[0][0] if domain_counts else 'None'\n",
    "    stats['Most Common Domain Count'] = domain_counts.most_common(1)[0][1] if domain_counts else 0\n",
    "    \n",
    "    # If this is the subfield graph, include field diversity\n",
    "    if 'field' in next(iter(G.nodes(data=True)))[1]:\n",
    "        fields = [data.get('field', 'Unknown') for _, data in G.nodes(data=True)]\n",
    "        field_counts = Counter(fields)\n",
    "        stats['Number of Fields'] = len(field_counts)\n",
    "        stats['Most Common Field'] = field_counts.most_common(1)[0][0] if field_counts else 'None'\n",
    "        stats['Most Common Field Count'] = field_counts.most_common(1)[0][1] if field_counts else 0\n",
    "    \n",
    "    # Top nodes by degree and count\n",
    "    node_degrees = dict(G.degree())\n",
    "    sorted_nodes_degree = sorted(node_degrees.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_nodes_degree = sorted_nodes_degree[:5]\n",
    "    \n",
    "    node_counts = {node: G.nodes[node].get('count', 0) for node in G.nodes()}\n",
    "    sorted_nodes_count = sorted(node_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_nodes_count = sorted_nodes_count[:5]\n",
    "    \n",
    "    # Convert to DataFrame for nice formatting\n",
    "    stats_df = pd.DataFrame(list(stats.items()), columns=['Metric', 'Value'])\n",
    "    \n",
    "    # Format the DataFrame\n",
    "    stats_df['Value'] = stats_df.apply(\n",
    "        lambda row: f\"{row['Value']:.4f}\" if isinstance(row['Value'], float) else row['Value'], \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Save statistics to CSV\n",
    "    stats_df.to_csv(f\"{title.replace(' ', '_')}_statistics.csv\", index=False)\n",
    "    \n",
    "    # Create a separate DataFrame for top nodes\n",
    "    top_nodes_df = pd.DataFrame({\n",
    "        'Top Nodes by Degree': [f\"{node} ({degree})\" for node, degree in top_nodes_degree],\n",
    "        'Top Nodes by Count': [f\"{node} ({count})\" for node, count in top_nodes_count]\n",
    "    })\n",
    "    \n",
    "    top_nodes_df.to_csv(f\"{title.replace(' ', '_')}_top_nodes.csv\", index=False)\n",
    "    \n",
    "    return stats_df, top_nodes_df\n",
    "\n",
    "# Function to generate LaTeX tables for academic papers\n",
    "def generate_latex_tables(stats_df, top_nodes_df, title):\n",
    "    \"\"\"\n",
    "    Generate LaTeX tables from the statistics DataFrames\n",
    "    \"\"\"\n",
    "    # Main statistics table\n",
    "    latex_stats = stats_df.to_latex(index=False, caption=f\"Network Statistics for {title}\", \n",
    "                               label=f\"tab:{title.replace(' ', '_').lower()}_stats\",\n",
    "                               longtable=True, escape=False)\n",
    "    \n",
    "    # Top nodes table\n",
    "    latex_top_nodes = top_nodes_df.to_latex(index=False, caption=f\"Top Nodes in {title} Network\", \n",
    "                                       label=f\"tab:{title.replace(' ', '_').lower()}_top_nodes\",\n",
    "                                       longtable=True, escape=False)\n",
    "    \n",
    "    # Save to files\n",
    "    with open(f\"{title.replace(' ', '_')}_statistics.tex\", 'w') as f:\n",
    "        f.write(latex_stats)\n",
    "        \n",
    "    with open(f\"{title.replace(' ', '_')}_top_nodes.tex\", 'w') as f:\n",
    "        f.write(latex_top_nodes)\n",
    "    \n",
    "    return latex_stats, latex_top_nodes\n",
    "\n",
    "def main():\n",
    "    # Load the articles\n",
    "    articles = load_data(\"all_articles.jsonl\")\n",
    "    print(f\"Loaded {len(articles)} articles\")\n",
    "    \n",
    "    # Create field graph\n",
    "    field_graph = create_field_graph(articles)\n",
    "    print(f\"Field graph created with {len(field_graph.nodes())} nodes and {len(field_graph.edges())} edges\")\n",
    "    \n",
    "    # Draw field graph\n",
    "    draw_network(field_graph, \"Field Connections\")\n",
    "    \n",
    "    # Generate statistics for field graph\n",
    "    field_stats_df, field_top_nodes_df = generate_network_statistics(field_graph, \"Field Connections\")\n",
    "    print(\"\\nField Network Statistics:\")\n",
    "    print(field_stats_df.to_string(index=False))\n",
    "    \n",
    "    print(\"\\nTop Nodes in Field Network:\")\n",
    "    print(field_top_nodes_df.to_string(index=False))\n",
    "    \n",
    "    # Generate top 20 fields table\n",
    "    top_fields_df = generate_top_nodes_table(field_graph, \"Fields\", top_n=20)\n",
    "    print(\"\\nTop 20 Fields by Frequency:\")\n",
    "    print(top_fields_df.to_string(index=False))\n",
    "    \n",
    "    # Generate LaTeX tables for field graph\n",
    "    generate_latex_tables(field_stats_df, field_top_nodes_df, \"Field Connections\")\n",
    "    \n",
    "    # Create subfield graph\n",
    "    subfield_graph = create_subfield_graph(articles)\n",
    "    print(f\"\\nSubfield graph created with {len(subfield_graph.nodes())} nodes and {len(subfield_graph.edges())} edges\")\n",
    "    \n",
    "    # Draw subfield graph\n",
    "    draw_network(subfield_graph, \"Subfield Connections\")\n",
    "    \n",
    "    # Generate statistics for subfield graph\n",
    "    subfield_stats_df, subfield_top_nodes_df = generate_network_statistics(subfield_graph, \"Subfield Connections\")\n",
    "    print(\"\\nSubfield Network Statistics:\")\n",
    "    print(subfield_stats_df.to_string(index=False))\n",
    "    \n",
    "    print(\"\\nTop Nodes in Subfield Network:\")\n",
    "    print(subfield_top_nodes_df.to_string(index=False))\n",
    "    \n",
    "    # Generate top 20 subfields table\n",
    "    top_subfields_df = generate_top_nodes_table(subfield_graph, \"Subfields\", top_n=20)\n",
    "    print(\"\\nTop 20 Subfields by Frequency:\")\n",
    "    print(top_subfields_df.to_string(index=False))\n",
    "    \n",
    "    # Generate LaTeX tables for subfield graph\n",
    "    generate_latex_tables(subfield_stats_df, subfield_top_nodes_df, \"Subfield Connections\")\n",
    "    \n",
    "    # Analyze interdisciplinary connections\n",
    "    print(\"\\nAnalyzing interdisciplinary connections...\")\n",
    "    \n",
    "    # Get domains for each node\n",
    "    field_domains = {node: data.get('domain', 'Unknown') \n",
    "                     for node, data in field_graph.nodes(data=True)}\n",
    "    \n",
    "    # Count cross-domain connections\n",
    "    cross_domain_edges = 0\n",
    "    domain_connections = {}\n",
    "    \n",
    "    for u, v in field_graph.edges():\n",
    "        domain_u = field_domains.get(u, 'Unknown')\n",
    "        domain_v = field_domains.get(v, 'Unknown')\n",
    "        \n",
    "        if domain_u != domain_v:\n",
    "            cross_domain_edges += 1\n",
    "            \n",
    "            # Track which domains are connecting\n",
    "            domain_pair = tuple(sorted([domain_u, domain_v]))\n",
    "            domain_connections[domain_pair] = domain_connections.get(domain_pair, 0) + 1\n",
    "    \n",
    "    # Calculate percentage of cross-domain connections\n",
    "    cross_domain_percentage = (cross_domain_edges / len(field_graph.edges())) * 100 if field_graph.edges() else 0\n",
    "    \n",
    "    print(f\"Cross-domain connections: {cross_domain_edges} ({cross_domain_percentage:.2f}% of all connections)\")\n",
    "    \n",
    "    # Top domain connections\n",
    "    top_domain_connections = sorted(domain_connections.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "    print(\"\\nTop domain connections:\")\n",
    "    for (domain1, domain2), count in top_domain_connections:\n",
    "        print(f\"  {domain1} <-> {domain2}: {count} connections\")\n",
    "    \n",
    "    # Create a summary table\n",
    "    summary_data = {\n",
    "        'Network Type': ['Field Network', 'Subfield Network'],\n",
    "        'Nodes': [len(field_graph.nodes()), len(subfield_graph.nodes())],\n",
    "        'Edges': [len(field_graph.edges()), len(subfield_graph.edges())],\n",
    "        'Density': [nx.density(field_graph), nx.density(subfield_graph)],\n",
    "        'Clustering Coefficient': [nx.average_clustering(field_graph), nx.average_clustering(subfield_graph)],\n",
    "        'Connected Components': [nx.number_connected_components(field_graph), nx.number_connected_components(subfield_graph)],\n",
    "        'Cross-Domain Edges %': [cross_domain_percentage, '-']\n",
    "    }\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    # Format the summary DataFrame\n",
    "    for col in ['Density', 'Clustering Coefficient', 'Cross-Domain Edges %']:\n",
    "        summary_df[col] = summary_df[col].apply(lambda x: f\"{x:.2f}%\" if isinstance(x, float) else x)\n",
    "    \n",
    "    print(\"\\nNetwork Comparison Summary:\")\n",
    "    print(summary_df.to_string(index=False))\n",
    "    \n",
    "    # Save summary to CSV and LaTeX\n",
    "    summary_df.to_csv(\"network_comparison_summary.csv\", index=False)\n",
    "    \n",
    "    latex_summary = summary_df.to_latex(index=False, caption=\"Comparison of Field and Subfield Networks\", \n",
    "                                   label=\"tab:network_comparison\", escape=False)\n",
    "    \n",
    "    with open(\"network_comparison_summary.tex\", 'w') as f:\n",
    "        f.write(latex_summary)\n",
    "    \n",
    "    print(\"\\nAll statistics have been saved to CSV and LaTeX files for use in academic publications.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dd5d24-56a6-44bf-b307-5e272a19f128",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (bgu)",
   "language": "python",
   "name": "bgu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
